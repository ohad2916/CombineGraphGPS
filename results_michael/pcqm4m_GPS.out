Starting job...
Activated environment: /home/yandex/MLWG2024/michaelbest/anaconda3/envs/graphgps
[*] Run ID 0: seed=0, split_index=0
    Starting now: 2024-11-11 12:36:08.368082
[*] Loaded dataset 'PCQM4Mv2-full' from 'OGB':
  Data(edge_index=[2, 109093626], edge_attr=[109093626, 3], x=[52970652, 9], y=[3746620])
  undirected: True
  num graphs: 3746620
  avg num_nodes/graph: 14
  num node features: 9
  num edge features: 3
  num classes: (appears to be a regression task)
Precomputing Positional Encoding statistics: ['LapPE'] for all graphs...
  ...estimated to be undirected: True
Done! Took 00:50:33.42
GraphGymModule(
  (model): GPSModel(
    (encoder): FeatureEncoder(
      (node_encoder): Concat2NodeEncoder(
        (encoder1): AtomEncoder(
          (atom_embedding_list): ModuleList(
            (0): Embedding(119, 296)
            (1): Embedding(5, 296)
            (2-3): 2 x Embedding(12, 296)
            (4): Embedding(10, 296)
            (5-6): 2 x Embedding(6, 296)
            (7-8): 2 x Embedding(2, 296)
          )
        )
        (encoder2): LapPENodeEncoder(
          (linear_A): Linear(in_features=2, out_features=16, bias=True)
          (pe_encoder): Sequential(
            (0): ReLU()
            (1): Linear(in_features=16, out_features=8, bias=True)
            (2): ReLU()
          )
        )
      )
      (edge_encoder): BondEncoder(
        (bond_embedding_list): ModuleList(
          (0): Embedding(5, 304)
          (1): Embedding(6, 304)
          (2): Embedding(2, 304)
        )
      )
    )
    (layers): Sequential(
      (0): GPSLayer(
        summary: dim_h=304, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=304, out_features=304, bias=True)
        )
        (norm1_local): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=304, out_features=608, bias=True)
        (ff_linear2): Linear(in_features=608, out_features=304, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): GPSLayer(
        summary: dim_h=304, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=304, out_features=304, bias=True)
        )
        (norm1_local): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=304, out_features=608, bias=True)
        (ff_linear2): Linear(in_features=608, out_features=304, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): GPSLayer(
        summary: dim_h=304, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=304, out_features=304, bias=True)
        )
        (norm1_local): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=304, out_features=608, bias=True)
        (ff_linear2): Linear(in_features=608, out_features=304, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): GPSLayer(
        summary: dim_h=304, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=304, out_features=304, bias=True)
        )
        (norm1_local): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=304, out_features=608, bias=True)
        (ff_linear2): Linear(in_features=608, out_features=304, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): GPSLayer(
        summary: dim_h=304, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=304, out_features=304, bias=True)
        )
        (norm1_local): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=304, out_features=608, bias=True)
        (ff_linear2): Linear(in_features=608, out_features=304, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): SANGraphHead(
      (FC_layers): ModuleList(
        (0): Linear(in_features=304, out_features=152, bias=True)
        (1): Linear(in_features=152, out_features=76, bias=True)
        (2): Linear(in_features=76, out_features=1, bias=True)
      )
      (activation): ReLU()
    )
  )
)
accelerator: cuda
benchmark: False
bn:
  eps: 1e-05
  mom: 0.1
cfg_dest: config.yaml
custom_metrics: []
dataset:
  cache_load: False
  cache_save: False
  dir: ./datasets
  edge_dim: 128
  edge_encoder: True
  edge_encoder_bn: False
  edge_encoder_name: Bond
  edge_encoder_num_types: 0
  edge_message_ratio: 0.8
  edge_negative_sampling_ratio: 1.0
  edge_train_mode: all
  encoder: True
  encoder_bn: True
  encoder_dim: 128
  encoder_name: db
  format: OGB
  infer_link_label: None
  label_column: none
  label_table: none
  location: local
  name: PCQM4Mv2-full
  node_encoder: True
  node_encoder_bn: False
  node_encoder_name: Atom+LapPE
  node_encoder_num_types: 0
  remove_feature: False
  resample_disjoint: False
  resample_negative: False
  shuffle_split: True
  slic_compactness: 10
  split: [0.8, 0.1, 0.1]
  split_dir: ./splits
  split_index: 0
  split_mode: standard
  task: graph
  task_type: regression
  to_undirected: False
  transductive: False
  transform: none
  tu_simple: True
devices: 1
example_arg: example
example_group:
  example_arg: example
gnn:
  act: relu
  agg: add
  att_final_linear: False
  att_final_linear_bn: False
  att_heads: 1
  batchnorm: True
  clear_feature: True
  dim_edge: 304
  dim_inner: 304
  dropout: 0.0
  head: san_graph
  keep_edge: 0.5
  l2norm: True
  layer_type: generalconv
  layers_mp: 2
  layers_post_mp: 3
  layers_pre_mp: 0
  msg_direction: single
  normalize_adj: False
  residual: False
  self_msg: concat
  skip_every: 1
  stage_type: stack
gpu_mem: False
graphormer:
  attention_dropout: 0.0
  dropout: 0.0
  embed_dim: 80
  input_dropout: 0.0
  mlp_dropout: 0.0
  num_heads: 4
  num_layers: 6
  use_graph_token: True
gt:
  attn_dropout: 0.5
  batch_norm: True
  bigbird:
    add_cross_attention: False
    attention_type: block_sparse
    block_size: 3
    chunk_size_feed_forward: 0
    hidden_act: relu
    is_decoder: False
    layer_norm_eps: 1e-06
    max_position_embeddings: 128
    num_random_blocks: 3
    use_bias: False
  dim_hidden: 304
  dropout: 0.0
  full_graph: True
  gamma: 1e-05
  layer_norm: False
  layer_type: CustomGatedGCN+Transformer
  layers: 5
  n_heads: 4
  pna_degrees: []
  residual: True
mem:
  inplace: False
metric_agg: argmin
metric_best: mae
model:
  edge_decoding: dot
  graph_pooling: mean
  loss_fun: l1
  match_upper: True
  size_average: mean
  thresh: 0.5
  type: GPSModel
name_tag: 
num_threads: 6
num_workers: 0
optim:
  base_lr: 0.0005
  batch_accumulation: 1
  clip_grad_norm: True
  clip_grad_norm_value: 1.0
  lr_decay: 0.1
  max_epoch: 100
  min_lr: 0.0
  momentum: 0.9
  num_warmup_epochs: 5
  optimizer: adamW
  reduce_factor: 0.1
  schedule_patience: 10
  scheduler: cosine_with_warmup
  steps: [30, 60, 90]
  weight_decay: 0.0
out_dir: results/pcqm4m-GPS
posenc_ElstaticSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: range(10)
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_EquivStableLapPE:
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  raw_norm_type: none
posenc_GraphormerBias:
  dim_pe: 0
  enable: False
  node_degrees_only: False
  num_in_degrees: None
  num_out_degrees: None
  num_spatial_types: None
posenc_HKdiagSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: 
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_LapPE:
  dim_pe: 8
  eigen:
    eigvec_norm: L2
    laplacian_norm: none
    max_freqs: 8
  enable: True
  layers: 2
  model: DeepSet
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_RWSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: 
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_SignNet:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: False
  phi_hidden_dim: 64
  phi_out_dim: 4
  post_layers: 0
  raw_norm_type: none
pretrained:
  dir: 
  freeze_main: False
  reset_prediction_head: True
print: both
round: 5
run_dir: results/pcqm4m-GPS/0
run_id: 0
run_multiple_splits: []
seed: 0
share:
  dim_in: 9
  dim_out: 1
  num_splits: 3
tensorboard_agg: True
tensorboard_each_run: False
train:
  auto_resume: False
  batch_size: 256
  ckpt_best: False
  ckpt_clean: True
  ckpt_period: 100
  enable_ckpt: True
  epoch_resume: -1
  eval_period: 1
  iter_per_epoch: 32
  mode: custom
  neighbor_sizes: [20, 15, 10, 5]
  node_per_graph: 32
  radius: extend
  sample_node: False
  sampler: full_batch
  skip_train_eval: False
  walk_length: 4
val:
  node_per_graph: 32
  radius: extend
  sample_node: False
  sampler: full_batch
view_emb: False
wandb:
  entity: gtransformers
  name: 
  project: pcqm4m
  use: False
Num parameters: 6154195
Start from epoch 0
Parameter containing:
tensor([1., 1.], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1., 1.], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1., 1.], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1., 1.], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1., 1.], device='cuda:0', requires_grad=True)
train: {'epoch': 0, 'time_epoch': 1528.52669, 'eta': 151324.14264, 'eta_hours': 42.03448, 'loss': 5.65506454, 'lr': 0.0, 'params': 6154195, 'time_iter': 0.1212, 'mae': 5.65506, 'r2': -23.66911, 'spearmanr': 0.15448, 'mse': 33.32367, 'rmse': 5.77267}
...computing epoch stats took: 1.17s
val: {'epoch': 0, 'time_epoch': 40.84593, 'loss': 5.65220994, 'lr': 0, 'params': 6154195, 'time_iter': 0.0697, 'mae': 5.65221, 'r2': -23.74709, 'spearmanr': 0.15961, 'mse': 33.28565, 'rmse': 5.76937}
...computing epoch stats took: 0.08s
test: {'epoch': 0, 'time_epoch': 20.24286, 'loss': 5.4109946, 'lr': 0, 'params': 6154195, 'time_iter': 0.07029, 'mae': 5.411, 'r2': -18.73548, 'spearmanr': 0.13229, 'mse': 30.83499, 'rmse': 5.55293}
...computing epoch stats took: 0.05s
> Epoch 0: took 1591.1s (avg 1591.1s) | Best so far: epoch 0	train_loss: 5.6551 train_mae: 5.6551	val_loss: 5.6522 val_mae: 5.6522	test_loss: 5.4110 test_mae: 5.4110
Parameter containing:
tensor([1., 1.], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1., 1.], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1., 1.], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1., 1.], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1., 1.], device='cuda:0', requires_grad=True)
train: {'epoch': 1, 'time_epoch': 1103.41247, 'eta': 128965.0188, 'eta_hours': 35.82362, 'loss': 0.23523594, 'lr': 0.0001, 'params': 6154195, 'time_iter': 0.08749, 'mae': 0.23524, 'r2': 0.87415, 'spearmanr': 0.95477, 'mse': 0.17, 'rmse': 0.41231}
...computing epoch stats took: 1.02s
val: {'epoch': 1, 'time_epoch': 21.19275, 'loss': 0.21153855, 'lr': 0, 'params': 6154195, 'time_iter': 0.03617, 'mae': 0.21154, 'r2': 0.93883, 'spearmanr': 0.9753, 'mse': 0.08227, 'rmse': 0.28684}
...computing epoch stats took: 0.04s
test: {'epoch': 1, 'time_epoch': 10.58009, 'loss': 0.2216299, 'lr': 0, 'params': 6154195, 'time_iter': 0.03674, 'mae': 0.22163, 'r2': 0.94044, 'spearmanr': 0.97406, 'mse': 0.09305, 'rmse': 0.30505}
...computing epoch stats took: 0.02s
> Epoch 1: took 1136.5s (avg 1363.8s) | Best so far: epoch 1	train_loss: 0.2352 train_mae: 0.2352	val_loss: 0.2115 val_mae: 0.2115	test_loss: 0.2216 test_mae: 0.2216
Parameter containing:
tensor([1.1433, 0.8567], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1.2919, 0.7081], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1.1903, 0.8097], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1.1026, 0.8974], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([1.0721, 0.9279], device='cuda:0', requires_grad=True)
train: {'epoch': 2, 'time_epoch': 1087.78011, 'eta': 120270.92294, 'eta_hours': 33.40859, 'loss': 0.17503946, 'lr': 0.0002, 'params': 6154195, 'time_iter': 0.08625, 'mae': 0.17504, 'r2': 0.95303, 'spearmanr': 0.97684, 'mse': 0.06344, 'rmse': 0.25188}
...computing epoch stats took: 0.99s
val: {'epoch': 2, 'time_epoch': 21.44876, 'loss': 0.16771291, 'lr': 0, 'params': 6154195, 'time_iter': 0.0366, 'mae': 0.16771, 'r2': 0.95716, 'spearmanr': 0.98085, 'mse': 0.05762, 'rmse': 0.24004}
...computing epoch stats took: 0.04s
test: {'epoch': 2, 'time_epoch': 10.59682, 'loss': 0.1787293, 'lr': 0, 'params': 6154195, 'time_iter': 0.03679, 'mae': 0.17873, 'r2': 0.95539, 'spearmanr': 0.97942, 'mse': 0.0697, 'rmse': 0.26401}
...computing epoch stats took: 0.03s
> Epoch 2: took 1121.1s (avg 1282.9s) | Best so far: epoch 2	train_loss: 0.1750 train_mae: 0.1750	val_loss: 0.1677 val_mae: 0.1677	test_loss: 0.1787 test_mae: 0.1787
